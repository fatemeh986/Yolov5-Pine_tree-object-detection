# Pine Tree Detection with YOLOv5

A lightweight pipeline for detecting pine trees in RGB images using YOLOv5. This repository covers:

* Converting Mask R-CNN JSON annotations to YOLO format
* Organizing a custom dataset for YOLOv5
* Running a series of training experiments
* Comparing results (mAP, recall, precision, confusion matrix)
* Analysis of performance and next steps

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ train
â”‚   â”‚   â”œâ”€â”€ images
â”‚   â”‚   â””â”€â”€ labels
â”‚   â””â”€â”€ validations
â”‚       â”œâ”€â”€ images
â”‚       â””â”€â”€ labels
â”œâ”€â”€ yolov5
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ val.py
â”‚   â”œâ”€â”€ detect.py
â”‚   â”œâ”€â”€ dataset.yml
â”‚   â””â”€â”€ â€¦ (YOLOv5 code & configs)
â”œâ”€â”€ convert_to_yolo.py        # Script to turn Mask R-CNN JSON â†’ YOLO TXT
â””â”€â”€ README.md
```

---

## ğŸ—‚ Dataset

* **One class**: `pine_tree` (class 0).

* **Structure** (root: `data/`):

  ```
  data/
  â”œâ”€ train/images
  â”œâ”€ train/labels    â† YOLO TXT files
  â”œâ”€ validations/images
  â””â”€ validations/labels
  ```

* **`yolov5/dataset.yml`**:

  ```yaml
  # root: yolov5/
  path: ../data
  train: train/images
  val:   validations/images
  nc:    1
  names:
    0: pine_tree
  ```

---

## ğŸ”„ Annotation Conversion

Use `convert_to_yolo.py` to turn your Mask R-CNN JSONs into YOLO `.txt` label files:

```bash
python convert_to_yolo.py \
  --json_dir data/train/annots \
  --labels_dir data/train/labels

python convert_to_yolo.py \
  --json_dir data/validations/annots \
  --labels_dir data/validations/labels
```

---

## âš™ï¸ Training Experiments

We ran **six** distinct experiments, each saved under `runs/train/<name>/`:

| Name            | Model    | Key Trick                                |
| --------------- | -------- | ---------------------------------------- |
| `pine_tree_run` | YOLOv5-s | default training (50 epochs)             |
| `exp_freeze`    | YOLOv5-s | first 30 ep freezing backbone â†’ +20 ep   |
| `exp_unfreeze`  | YOLOv5-s | fine-tune all layers                     |
| `exp_anchor`    | YOLOv5-s | (auto)recomputed anchors                 |
| `exp_evolve`    | YOLOv5-s | hyperparameter evolution (`--evolve`)    |
| `exp_yolov5m2`  | YOLOv5-m | medium model + best `exp_evolve` hparams |

Each was launched with a one-liner, e.g.:

```bash
python yolov5/train.py \
  --weights yolov5s.pt \
  --data dataset.yml \
  --img 640 \
  --batch 16 \
  --epochs 50 \
  --name exp_anchor
```

---

## ğŸ“Š Results

### Quantitative Metrics

| Experiment      | Precision | Recall | mAP\@50 | mAP\@50-95 | â€œAccuracyâ€ (TP/(TP+FP+FN)) |
| --------------- | :-------: | :----: | :-----: | :--------: | :------------------------: |
| pine\_tree\_run |   0.664   |  0.600 |  0.646  |    0.417   |           46.2 %           |
| exp\_yolov5m2   |     â€”     |  0.830 |    â€”    |      â€”     |           41.5 %           |

*(Detailed logs are in each runâ€™s `results.txt`.)*

### Best Confusion Matrix

![](runs/val/val_exp_yolov5m2/confusion_matrix.png)

* **True Positive Rate** (recall): 83 %
* **False Negative Rate**: 17 %
* **False Positive Rate** (backgroundâ†’pine): 100 %
* Implicit background class yields a 2Ã—2 matrix for singleâ€class detection.

---

## ğŸ–¼ï¸ Example Detections

Here are two sample detection outputs:

![Detection Example 1](https://imgur.com/a/wsoGKaL)

<!-- ![Detection Example 2](val_batch0_pred.jpg) -->


## ğŸ” Analysis

1. **Dataset Challenges**

   * **Size & diversity**: only \~150 train images with limited backgrounds
   * **Image quality**: some images are low-contrast or blurred
   * **Singleâ€class, implicit negatives**: no explicit â€œbackgroundâ€ boxes

2. **Model Capacity & Augmentation**

   * Upgrading to YOLOv5-m and applying evolved hyperparameters boosted recall from 60 % â†’ 83 %.
   * Precision remains limited by high false positives on complex backgrounds.

3. **Detectionâ€Accuracy vs. Classification**

   * In object detection, we measure precision/recall/mAP; the â€œaccuracyâ€ here is TP/(TP+FP+FN) â‰ˆ 41 %.

---

## ğŸš€ Next Steps

* **Expand & diversify data**: collect more scenes (different lighting, angles, seasons).
* **Improve annotations**: tighter, more consistent bounding boxes; consider adding a few explicit â€œbackgroundâ€ crops for negative sampling.
* **Multi-spectral inputs**: include NIR or depth channels if available.
* **Advanced augmentation**: copy-paste, stronger color/scale/perspective variants.
* **Ensemble & TTA**: combine multiple runs or test-time augmentations for gain.

---

## â–¶ï¸ Usage

1. **Install dependencies**

   ```bash
   pip install -r yolov5/requirements.txt
   ```
2. **Convert annotations**

   ```bash
   python convert_to_yolo.py ...
   ```
3. **Train**

   ```bash
   python yolov5/train.py --data yolov5/dataset.yml --name your_experiment
   ```
4. **Validate**

   ```bash
   python yolov5/val.py --weights runs/train/your_experiment/weights/best.pt \
                        --data yolov5/dataset.yml \
                        --save-conf --verbose --name val_your_experiment
   ```
5. **Detect**

   ```bash
   python yolov5/detect.py --weights runs/train/your_experiment/weights/best.pt \
                          --source path/to/images \
                          --save-txt --name detect_your_experiment
   ```

---

> **Acknowledgments**
>
> * [Ultralytics YOLOv5](https://github.com/ultralytics/yolov5)
> * Early experiments with Mask R-CNN annotations
